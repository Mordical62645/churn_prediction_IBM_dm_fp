# -*- coding: utf-8 -*-
"""churn_prediction_IBM_dm_fp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fl-T_pbyzQ-WZqjpHe3Q7uXUIhCTBQzx
"""

# !gdown --fuzzy "https://drive.google.com/file/d/1rYFumAaLcacQb59IYC-g_8douKWOIkTi/view?usp=sharing"

#### LIBRARIES ####
import pandas as pd
import numpy as np

# For splitting and scaling
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# For modeling and evaluation
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix

# For visualization
import matplotlib.pyplot as plt
import seaborn as sns

# For handling imbalanced datasets
from imblearn.over_sampling import SMOTE

# For high-performance models
import xgboost as xgb

# Optional: for building a simple interactive web app
import streamlit as st

#### DATASET ####
# Load CSV
file_path = "IBM.csv"
df = pd.read_csv(file_path)
print(df.head()) # check column contents
print("-"*60,"\n")

#### DATA CLEANING ####
# check for column datatypes
print(df.info()) # TotalCharges' data type is string, which is weird. Blank strings might be the reason why it cannot be converted to float.
print("-"*60,"\n")

# work with missing values first because blank 'strings' cannot be converted to float
print(df.isnull().sum()) # blank strings can be classified as not null.
print("-"*60,"\n")

# replace blank strings with NaN
df['TotalCharges'] = df['TotalCharges'].replace(" ", np.nan)
print(df.isnull().sum()) # recheck missing values.
print("-"*60,"\n")

# convert TotalCharges to float
df['TotalCharges'] = df['TotalCharges'].astype(float)
print(df.info()) # recheck datatype
print("-"*60,"\n")

# Missing TotalCharges: 11 (less missing data are safe to drop, BRAD)
df = df.dropna() # drop rows with missing (NaN) values: TotalCharges
print(df.isnull().sum()) # check missing
print("-"*60,"\n")

# We'll temprorarily drop irrelevant columns for the model. This will reduce training load for the model to handle.
# customerID is purely unique identifier. It's safe to drop.
df = df.drop('customerID', axis = 1) # axis = 0: delete a row; axis = 1 delete a column

# work with redundant data. I'll just use AI to check for that shit with 7000 rows motherfucker that's a lot to analyze.
# According to my boy, BaiGPT. Mahimo natong i-standardize ang 'No phone service' ug 'No internet service' isip 'No' hinuon.
# Let's just use "No" for 'No internet service' and 'No phone service'.
df = df.replace({
    'MultipleLines': {'No phone service':'No'},
    'OnlineSecurity': {'No internet service':'No'},
    'OnlineBackup': {'No internet service':'No'},
    'DeviceProtection': {'No internet service':'No'},
    'TechSupport': {'No internet service':'No'},
    'StreamingTV': {'No internet service':'No'},
    'StreamingMovies': {'No internet service':'No'},

    # tinatamad na ako mag-ingles perd. May mga areas na okay naman like 'Bank transfer (automatic)' tsaka 'Credit card (automatic)'
    # pwede naman na tanggaling yung '(automatic)' part.
    # ganun din sa 'Contract' column. Yung casing lang like 'Month-To-Month' ganern!
    'PaymentMethod': {
        'Bank transfer (automatic)': 'Bank Transfer',
        'Credit card (automatic)': 'Credit Card'
    },
    'Contract': {
        'Month-to-month': 'Month-To-Month',
        'One year': 'One Year',
        'Two year': 'Two Year'
    }
})

#### HANDLE CLASS IMBALANCE + TRAIN TEST SPLIT ####
# Map (not replace) numeric of 'Churn' instead of Yes or No, we'll do 1 or 0
df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})
df_encoded = pd.get_dummies(df) # Turn categorical data to numeric

# Define the features (X) and labels (y).
X = df_encoded.drop(columns=['Churn'])
# print(X)
y = df_encoded['Churn']
# print(y)

# Use train_test_split() to divide into training and test sets.
  # X_train: Features for training
  # X_test: Features for testing
  # y_train: Labels for training
  # y_test: Labels for testing
  # test_size is how much data goes into test set (denoted by 0.2 = 20%)
  # random_state is the seed so results is reproducible. No seed = random
  # stratify=y ensures the class distribution in y is preserved in both the train and test sets.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
  # 80% of data goes to training.
  # 20% goes to testing.
  # The proportion of churned vs. non-churned customers is the same in both sets.
  # The split will always be the same every time you run it (because of random_state=42).

# Apply SMOTE only to X_train and y_train.
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Print value_counts() to confirm class balance.
print(f"Before SMOTE:\n {y_train.value_counts()}")
print("-"*60,"\n")
print(f"Afer SMOTE:\n {y_train_resampled.value_counts()}")

#### FEATURE SCALING ####
# Apply StandardScaler() or similar only to features (X), not labels (y).


# Remember: fit on X_train, then transform both X_train and X_test.

#### MODELLING ####
# LogisticRegression


# XGBoost


# Fit the model using X_train and y_train

#### EVALUATION ####
# Predict on X_test.
# Evaluate using:
# Accuracy


# Confusion matrix


# Classification report (precision, recall, F1-score)

#### PREDICTION AND OUTPUT ####